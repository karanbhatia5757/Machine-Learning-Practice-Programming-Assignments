{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing a Fixed Mathematical Function Using Tensorflow:\n",
    "y = x^2 + 5x + 10\n",
    "The parameter(beta or weight in case of a neural network) for which we have to find optimized value has to be declared as a tensorflow variable. eg: Here, x has to be declared as a tensorflow variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declaring x as a variable with initial value as 0 and datatype as 64 bit floating number.\n",
    "x = tf.Variable(0 , dtype = tf.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writing the mathematical function (cost function in machine learning) which we need to minimize.\n",
    "# There are 2 ways to write this function.\n",
    "# y = tf.add(tf.add(x**2 , tf.multiply(5.0,x)), 10)\n",
    "# Operators are already overloaded ,so we can write the expression in the following way.\n",
    "y = x**2 + 5*x + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Algorithm that will minimize the function y:\n",
    "# Gradient Descent with learning rate = 0.1  \n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the global variables.\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Till now, we haven't executed anything. We have just defined the things to be done. \n",
    "# To run our program, we need to create a session.\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running the program with the help of sess object:\n",
    "# Firstly, we have to run the init i.e. initialize the global variables,\n",
    "# then only we can run things using the variables.\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the variable x:\n",
    "sess.run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running the gradient descent algorithm for 1 iteration:\n",
    "sess.run(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the value of x after 1 iteration of Gradient Descent Algorithm:\n",
    "sess.run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running the Gradient Descent Algorithm for many iterations using a for loop:\n",
    "for i in range(100):\n",
    "    sess.run(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.4999995"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the final value of parameter. \n",
    "sess.run(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tensorflow to minimize a cost function that is not fixed and uses data.\n",
    "In the above program, we have minimized a cost function that is fixed. Let's see how we can minimize a changing cost function i.e. when the coefficient of the equation can change.<br>\n",
    "Now, instead of providing fixed coefficient to the function y as 1, 5, 10, we would provide a way so that we can change the co-efficients. For this, we use the concept of tensorflow placeholder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow Placeholder: It is an object whose value we can pass during session.run(). See the program.\n",
    "# Creating a placeholder of shape i.e. an array [3,1] with datatype as float64 \n",
    "x = tf.placeholder(tf.float64 , [3,1])\n",
    "# Also, we need to have a variable. Let us create a new variable 'w'\n",
    "w = tf.Variable( 0 , dtype = tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to be optimized.\n",
    "y = x[0][0] * w ** 2 + x[1][0] * w + x[2][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the algorithm that will minimize the function 'y':\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a numpy array of shape (3,1) , whose values we will use to feed x:\n",
    "coefficients = np.array([[5] , [7], [100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declaring Global Variable Initializer\n",
    "init1 = tf.global_variables_initializer()\n",
    "# Creating Session object\n",
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running init1 \n",
    "session.run(init1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Feed cannot be a tensor object. It can be an object like numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here, we will provide the value of x using coefficients numpy array.\n",
    "session.run(train , feed_dict = {x : coefficients })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.70000001043081284"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running Train for many iterations:\n",
    "for i in range(100):\n",
    "    session.run(train , feed_dict = {x:coefficients })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.69999999999999996"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network Using Tensorflow.\n",
    "Now, we will use Tensorflow to train a Neural Network. In a Neural Network, we have 2 main steps:<br>\n",
    "1) Forward Propagation <br>\n",
    "2) Backward Propagation <br>\n",
    "\n",
    "We do forward propagation, calculate cost function , and then do backpropagation to calculate derivative of cost function w.r.t parameters and finally update the parameters using Gradient Descent. <br>\n",
    "As you have already seen that doing backpropagation even in a simple Neural Network can be tricky or difficult. Deep Learning Frameworks like Tensorflow makes it easier for us. <br>\n",
    "We only write the forward propagation, calculate the cost function and rest of the work is done by Tensorflow!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us use the cancer dataset as we did in assignment 4 and learn a classifier using a Neural Network \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading and preprocessing the dataset. Same as done in Assignment 4 \n",
    "cancer=load_breast_cancer()\n",
    "x_scale =  preprocessing.scale(cancer.data)\n",
    "y = cancer.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scale, cancer.target, stratify= cancer.target, random_state=0)\n",
    "x_train = x_train.T\n",
    "y_train = y_train.reshape((1 ,y_train.shape[0]))\n",
    "x_test = x_test.T\n",
    "y_test = y_test.reshape((1,y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 143)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us see the datasets.\n",
    "x_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note the shape of data set : A column represents an example and row represents a feature. \n",
    "This notation helps. This helps in vectorized implementation i.e. we need not use a for loop to forward propagate all examples. We can do this using a single line of code.We will see this later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Placeholders for training data:\n",
    "Remember that training data will always be in a placeholders and actual values will be passed when we run the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use none because we can change the number of training examples to pass to the optimizer.\n",
    "# If we are doing batch gradient descent, we will pass all examples or pass 1 example in an iteration when doing stochastic gradient descent.\n",
    "X = tf.placeholder(tf.float32, [x_train.shape[0], None], name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = tf.placeholder(tf.float32, [y_train.shape[0], None], name=\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter/Weights Initialization\n",
    "Remember that the parameters/weights have to be Tensorflow Variables.<br>\n",
    "We should not initialize W as zeros. We can initialize b as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.get_variable(\"W1\", [30, 30], initializer = tf.random_normal_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b1 = tf.get_variable(\"b1\", [30, 1], initializer = tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2 = tf.get_variable(\"W2\", [30, 30], initializer = tf.random_normal_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b2 = tf.get_variable(\"b2\" , [30,1] ,  initializer = tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W3 = tf.get_variable(\"W3\" , [1,30] , initializer = tf.random_normal_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b3 = tf.get_variable(\"b3\" , [1,1] , initializer = tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Part of First Hidden Layer, Values of 'b' will automatically broadcasted according to the number of examples. \n",
    "\n",
    "Z1 = tf.add(tf.matmul(W1, X), b1)                         # Same as Z1 = np.dot(W1, X) + b1\n",
    "# Applying Sigmoid Activation on Z1\n",
    "A1 = tf.nn.sigmoid(Z1)                                    # Same as A1 = sigmoid(Z1)\n",
    "# Linear Part of Second Hidden Layer  \n",
    "Z2 = tf.add(tf.matmul(W2, A1), b2)                        # Same as Z2 = np.dot(W2, a1) + b2\n",
    "# Applying Sigmoid Activation on Z1\n",
    "A2 = tf.nn.sigmoid(Z2)                                    # Same as A2 = sigmoid(Z2)\n",
    "# Output Node\n",
    "Z3 = tf.add(tf.matmul(W3, A2), b3)\n",
    "# Sigmoid of A3\n",
    "A3 = tf.nn.sigmoid(Z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the cost\n",
    "Now, we have to compute cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = -(tf.reduce_sum(Y * tf.log(A3)) + tf.reduce_sum((1-Y)*tf.log(1-A3)))/y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the optimizer:\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.02).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializzing all the global variables.\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session_1 = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session_1.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session_1.run(optimizer , feed_dict={X:x_train,Y:y_train })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us see how the cost function gets optimized by plotting the cost:\n",
    "# Initialize a python list to store cost value\n",
    "cost_plot = []\n",
    "for i in range(10000):\n",
    "    session_1.run(optimizer , feed_dict={X:x_train,Y:y_train })\n",
    "    cost_value = session_1.run(cost , feed_dict={X:x_train,Y:y_train })\n",
    "    cost_plot.append(cost_value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iters = np.array(range(0,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x126cd1f60>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndxGROqRwFxGpQwp3EZE6pHAXEalD/x/wGECl50MKbwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125c49400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iters,cost_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.2728212,\n",
       " 2.2277806,\n",
       " 2.1828883,\n",
       " 2.1381502,\n",
       " 2.0935795,\n",
       " 2.0491862,\n",
       " 2.0049825,\n",
       " 1.9609834,\n",
       " 1.9172014,\n",
       " 1.8736538,\n",
       " 1.830356,\n",
       " 1.7873274,\n",
       " 1.7445874,\n",
       " 1.7021577,\n",
       " 1.6600616,\n",
       " 1.6183236,\n",
       " 1.5769701,\n",
       " 1.5360298,\n",
       " 1.4955316,\n",
       " 1.4555075,\n",
       " 1.4159907,\n",
       " 1.3770155,\n",
       " 1.3386179,\n",
       " 1.3008343,\n",
       " 1.2637028,\n",
       " 1.2272607,\n",
       " 1.191547,\n",
       " 1.1565992,\n",
       " 1.1224539,\n",
       " 1.089147,\n",
       " 1.0567125,\n",
       " 1.025182,\n",
       " 0.99458426,\n",
       " 0.96494478,\n",
       " 0.9362852,\n",
       " 0.90862364,\n",
       " 0.88197345,\n",
       " 0.85634339,\n",
       " 0.83173764,\n",
       " 0.80815578,\n",
       " 0.78559238,\n",
       " 0.76403791,\n",
       " 0.74347818,\n",
       " 0.72389531,\n",
       " 0.70526731,\n",
       " 0.68756956,\n",
       " 0.67077416,\n",
       " 0.6548509,\n",
       " 0.63976771,\n",
       " 0.62549096,\n",
       " 0.61198604,\n",
       " 0.59921747,\n",
       " 0.58714956,\n",
       " 0.57574689,\n",
       " 0.56497389,\n",
       " 0.55479562,\n",
       " 0.54517829,\n",
       " 0.53608888,\n",
       " 0.52749527,\n",
       " 0.51936686,\n",
       " 0.51167423,\n",
       " 0.50438929,\n",
       " 0.49748543,\n",
       " 0.49093729,\n",
       " 0.48472115,\n",
       " 0.47881451,\n",
       " 0.47319633,\n",
       " 0.46784669,\n",
       " 0.46274728,\n",
       " 0.45788077,\n",
       " 0.45323104,\n",
       " 0.44878316,\n",
       " 0.44452325,\n",
       " 0.4404383,\n",
       " 0.43651631,\n",
       " 0.43274629,\n",
       " 0.42911792,\n",
       " 0.42562172,\n",
       " 0.42224878,\n",
       " 0.41899106,\n",
       " 0.41584107,\n",
       " 0.41279188,\n",
       " 0.40983704,\n",
       " 0.40697071,\n",
       " 0.40418741,\n",
       " 0.40148211,\n",
       " 0.39885017,\n",
       " 0.39628729,\n",
       " 0.39378953,\n",
       " 0.39135319,\n",
       " 0.38897496,\n",
       " 0.38665164,\n",
       " 0.38438043,\n",
       " 0.38215864,\n",
       " 0.37998369,\n",
       " 0.37785336,\n",
       " 0.37576559,\n",
       " 0.37371835,\n",
       " 0.37170982,\n",
       " 0.36973828,\n",
       " 0.36780214,\n",
       " 0.36590001,\n",
       " 0.36403045,\n",
       " 0.36219218,\n",
       " 0.36038408,\n",
       " 0.35860494,\n",
       " 0.35685378,\n",
       " 0.35512963,\n",
       " 0.35343155,\n",
       " 0.35175869,\n",
       " 0.35011023,\n",
       " 0.34848538,\n",
       " 0.34688354,\n",
       " 0.34530392,\n",
       " 0.34374592,\n",
       " 0.34220898,\n",
       " 0.3406924,\n",
       " 0.33919579,\n",
       " 0.33771852,\n",
       " 0.33626023,\n",
       " 0.33482033,\n",
       " 0.33339843,\n",
       " 0.33199409,\n",
       " 0.33060691,\n",
       " 0.32923654,\n",
       " 0.32788259,\n",
       " 0.32654464,\n",
       " 0.32522246,\n",
       " 0.32391566,\n",
       " 0.32262394,\n",
       " 0.321347,\n",
       " 0.32008451,\n",
       " 0.31883627,\n",
       " 0.31760195,\n",
       " 0.31638128,\n",
       " 0.3151741,\n",
       " 0.31398001,\n",
       " 0.31279889,\n",
       " 0.31163052,\n",
       " 0.31047457,\n",
       " 0.30933091,\n",
       " 0.30819929,\n",
       " 0.30707955,\n",
       " 0.30597147,\n",
       " 0.30487481,\n",
       " 0.30378947,\n",
       " 0.30271518,\n",
       " 0.30165187,\n",
       " 0.30059925,\n",
       " 0.29955721,\n",
       " 0.29852557,\n",
       " 0.29750419,\n",
       " 0.29649287,\n",
       " 0.29549152,\n",
       " 0.29449993,\n",
       " 0.29351798,\n",
       " 0.29254553,\n",
       " 0.29158238,\n",
       " 0.29062846,\n",
       " 0.28968364,\n",
       " 0.28874773,\n",
       " 0.28782067,\n",
       " 0.28690231,\n",
       " 0.28599247,\n",
       " 0.28509107,\n",
       " 0.28419799,\n",
       " 0.28331316,\n",
       " 0.2824364,\n",
       " 0.2815676,\n",
       " 0.28070667,\n",
       " 0.27985349,\n",
       " 0.27900803,\n",
       " 0.27817002,\n",
       " 0.27733952,\n",
       " 0.27651635,\n",
       " 0.27570045,\n",
       " 0.27489164,\n",
       " 0.27408993,\n",
       " 0.27329516,\n",
       " 0.27250731,\n",
       " 0.27172619,\n",
       " 0.27095178,\n",
       " 0.27018398,\n",
       " 0.26942268,\n",
       " 0.26866785,\n",
       " 0.26791933,\n",
       " 0.26717708,\n",
       " 0.26644102,\n",
       " 0.2657111,\n",
       " 0.26498717,\n",
       " 0.26426923,\n",
       " 0.26355717,\n",
       " 0.26285088,\n",
       " 0.26215038,\n",
       " 0.26145551,\n",
       " 0.26076624,\n",
       " 0.26008251,\n",
       " 0.25940421,\n",
       " 0.25873134,\n",
       " 0.25806379,\n",
       " 0.2574015,\n",
       " 0.25674441,\n",
       " 0.25609243,\n",
       " 0.25544554,\n",
       " 0.25480366,\n",
       " 0.25416675,\n",
       " 0.2535347,\n",
       " 0.25290751,\n",
       " 0.25228512,\n",
       " 0.25166744,\n",
       " 0.25105441,\n",
       " 0.25044602,\n",
       " 0.24984215,\n",
       " 0.2492428,\n",
       " 0.24864791,\n",
       " 0.24805744,\n",
       " 0.24747132,\n",
       " 0.2468895,\n",
       " 0.24631196,\n",
       " 0.2457386,\n",
       " 0.24516939,\n",
       " 0.24460429,\n",
       " 0.24404323,\n",
       " 0.24348624,\n",
       " 0.24293321,\n",
       " 0.24238409,\n",
       " 0.24183887,\n",
       " 0.24129751,\n",
       " 0.24075995,\n",
       " 0.24022613,\n",
       " 0.23969606,\n",
       " 0.23916967,\n",
       " 0.23864689,\n",
       " 0.23812772,\n",
       " 0.23761208,\n",
       " 0.23710002,\n",
       " 0.2365914,\n",
       " 0.23608625,\n",
       " 0.2355845,\n",
       " 0.23508613,\n",
       " 0.23459107,\n",
       " 0.23409934,\n",
       " 0.23361088,\n",
       " 0.23312564,\n",
       " 0.23264363,\n",
       " 0.23216477,\n",
       " 0.23168902,\n",
       " 0.23121639,\n",
       " 0.23074684,\n",
       " 0.23028031,\n",
       " 0.22981678,\n",
       " 0.22935624,\n",
       " 0.22889866,\n",
       " 0.228444,\n",
       " 0.22799218,\n",
       " 0.22754323,\n",
       " 0.22709714,\n",
       " 0.2266538,\n",
       " 0.22621329,\n",
       " 0.2257755,\n",
       " 0.2253404,\n",
       " 0.22490804,\n",
       " 0.22447832,\n",
       " 0.22405121,\n",
       " 0.22362673,\n",
       " 0.22320484,\n",
       " 0.22278549,\n",
       " 0.2223687,\n",
       " 0.22195444,\n",
       " 0.22154261,\n",
       " 0.22113328,\n",
       " 0.22072637,\n",
       " 0.22032189,\n",
       " 0.2199198,\n",
       " 0.21952009,\n",
       " 0.21912274,\n",
       " 0.21872765,\n",
       " 0.2183349,\n",
       " 0.21794444,\n",
       " 0.21755622,\n",
       " 0.21717025,\n",
       " 0.21678653,\n",
       " 0.21640494,\n",
       " 0.21602556,\n",
       " 0.21564835,\n",
       " 0.21527326,\n",
       " 0.21490027,\n",
       " 0.21452937,\n",
       " 0.21416058,\n",
       " 0.21379383,\n",
       " 0.21342914,\n",
       " 0.21306646,\n",
       " 0.21270579,\n",
       " 0.21234708,\n",
       " 0.2119904,\n",
       " 0.21163559,\n",
       " 0.21128275,\n",
       " 0.21093184,\n",
       " 0.21058281,\n",
       " 0.21023567,\n",
       " 0.20989037,\n",
       " 0.20954695,\n",
       " 0.20920534,\n",
       " 0.20886555,\n",
       " 0.20852757,\n",
       " 0.20819135,\n",
       " 0.20785691,\n",
       " 0.20752423,\n",
       " 0.2071933,\n",
       " 0.20686407,\n",
       " 0.20653653,\n",
       " 0.20621075,\n",
       " 0.20588663,\n",
       " 0.20556411,\n",
       " 0.20524329,\n",
       " 0.20492411,\n",
       " 0.20460653,\n",
       " 0.2042906,\n",
       " 0.20397623,\n",
       " 0.20366347,\n",
       " 0.20335227,\n",
       " 0.2030426,\n",
       " 0.2027345,\n",
       " 0.20242789,\n",
       " 0.20212287,\n",
       " 0.2018193,\n",
       " 0.20151724,\n",
       " 0.20121665,\n",
       " 0.20091753,\n",
       " 0.20061986,\n",
       " 0.20032364,\n",
       " 0.20002887,\n",
       " 0.19973551,\n",
       " 0.19944355,\n",
       " 0.19915299,\n",
       " 0.19886385,\n",
       " 0.19857606,\n",
       " 0.19828962,\n",
       " 0.19800454,\n",
       " 0.19772081,\n",
       " 0.19743842,\n",
       " 0.19715735,\n",
       " 0.19687757,\n",
       " 0.19659911,\n",
       " 0.19632195,\n",
       " 0.19604607,\n",
       " 0.19577146,\n",
       " 0.19549808,\n",
       " 0.19522597,\n",
       " 0.19495511,\n",
       " 0.19468549,\n",
       " 0.19441707,\n",
       " 0.19414988,\n",
       " 0.19388387,\n",
       " 0.1936191,\n",
       " 0.19335547,\n",
       " 0.19309305,\n",
       " 0.1928318,\n",
       " 0.19257167,\n",
       " 0.19231272,\n",
       " 0.1920549,\n",
       " 0.1917982,\n",
       " 0.19154266,\n",
       " 0.19128819,\n",
       " 0.19103488,\n",
       " 0.19078265,\n",
       " 0.19053151,\n",
       " 0.19028144,\n",
       " 0.19003244,\n",
       " 0.18978453,\n",
       " 0.18953769,\n",
       " 0.18929186,\n",
       " 0.18904708,\n",
       " 0.18880337,\n",
       " 0.18856066,\n",
       " 0.18831901,\n",
       " 0.18807837,\n",
       " 0.18783867,\n",
       " 0.18760005,\n",
       " 0.18736239,\n",
       " 0.1871257,\n",
       " 0.18688999,\n",
       " 0.18665525,\n",
       " 0.1864215,\n",
       " 0.18618871,\n",
       " 0.18595687,\n",
       " 0.18572591,\n",
       " 0.18549596,\n",
       " 0.18526691,\n",
       " 0.18503879,\n",
       " 0.18481159,\n",
       " 0.18458532,\n",
       " 0.18435992,\n",
       " 0.18413542,\n",
       " 0.18391183,\n",
       " 0.18368913,\n",
       " 0.18346733,\n",
       " 0.18324636,\n",
       " 0.1830263,\n",
       " 0.18280706,\n",
       " 0.18258871,\n",
       " 0.18237118,\n",
       " 0.18215452,\n",
       " 0.18193871,\n",
       " 0.18172371,\n",
       " 0.18150952,\n",
       " 0.18129621,\n",
       " 0.18108368,\n",
       " 0.18087196,\n",
       " 0.18066105,\n",
       " 0.18045095,\n",
       " 0.18024161,\n",
       " 0.18003312,\n",
       " 0.17982537,\n",
       " 0.17961843,\n",
       " 0.17941223,\n",
       " 0.17920683,\n",
       " 0.17900217,\n",
       " 0.17879829,\n",
       " 0.17859516,\n",
       " 0.17839277,\n",
       " 0.17819114,\n",
       " 0.17799023,\n",
       " 0.17779008,\n",
       " 0.17759064,\n",
       " 0.17739193,\n",
       " 0.17719394,\n",
       " 0.17699669,\n",
       " 0.17680012,\n",
       " 0.17660426,\n",
       " 0.17640913,\n",
       " 0.17621469,\n",
       " 0.17602095,\n",
       " 0.17582789,\n",
       " 0.17563549,\n",
       " 0.1754438,\n",
       " 0.17525278,\n",
       " 0.17506245,\n",
       " 0.17487279,\n",
       " 0.17468373,\n",
       " 0.1744954,\n",
       " 0.1743077,\n",
       " 0.17412066,\n",
       " 0.17393424,\n",
       " 0.17374854,\n",
       " 0.17356339,\n",
       " 0.17337893,\n",
       " 0.17319506,\n",
       " 0.17301185,\n",
       " 0.17282929,\n",
       " 0.17264731,\n",
       " 0.17246598,\n",
       " 0.17228523,\n",
       " 0.1721051,\n",
       " 0.17192557,\n",
       " 0.17174667,\n",
       " 0.17156833,\n",
       " 0.17139062,\n",
       " 0.17121348,\n",
       " 0.17103694,\n",
       " 0.17086098,\n",
       " 0.1706856,\n",
       " 0.17051081,\n",
       " 0.17033659,\n",
       " 0.17016293,\n",
       " 0.16998985,\n",
       " 0.16981734,\n",
       " 0.1696454,\n",
       " 0.16947396,\n",
       " 0.1693031,\n",
       " 0.16913284,\n",
       " 0.16896307,\n",
       " 0.16879389,\n",
       " 0.16862524,\n",
       " 0.16845714,\n",
       " 0.16828954,\n",
       " 0.16812247,\n",
       " 0.16795596,\n",
       " 0.16778997,\n",
       " 0.1676245,\n",
       " 0.16745955,\n",
       " 0.16729511,\n",
       " 0.1671312,\n",
       " 0.16696779,\n",
       " 0.16680489,\n",
       " 0.16664253,\n",
       " 0.16648063,\n",
       " 0.16631927,\n",
       " 0.16615838,\n",
       " 0.16599798,\n",
       " 0.16583808,\n",
       " 0.16567868,\n",
       " 0.16551976,\n",
       " 0.16536132,\n",
       " 0.16520335,\n",
       " 0.16504587,\n",
       " 0.16488887,\n",
       " 0.16473235,\n",
       " 0.16457631,\n",
       " 0.16442072,\n",
       " 0.16426559,\n",
       " 0.16411096,\n",
       " 0.16395675,\n",
       " 0.16380301,\n",
       " 0.16364972,\n",
       " 0.16349693,\n",
       " 0.16334455,\n",
       " 0.16319263,\n",
       " 0.16304113,\n",
       " 0.16289008,\n",
       " 0.16273952,\n",
       " 0.16258937,\n",
       " 0.16243966,\n",
       " 0.16229036,\n",
       " 0.16214155,\n",
       " 0.16199312,\n",
       " 0.1618451,\n",
       " 0.16169757,\n",
       " 0.16155043,\n",
       " 0.1614037,\n",
       " 0.16125742,\n",
       " 0.1611115,\n",
       " 0.16096605,\n",
       " 0.16082101,\n",
       " 0.16067635,\n",
       " 0.16053213,\n",
       " 0.16038831,\n",
       " 0.16024487,\n",
       " 0.16010188,\n",
       " 0.15995924,\n",
       " 0.15981703,\n",
       " 0.15967521,\n",
       " 0.15953377,\n",
       " 0.15939274,\n",
       " 0.15925209,\n",
       " 0.15911183,\n",
       " 0.15897194,\n",
       " 0.15883248,\n",
       " 0.15869337,\n",
       " 0.15855466,\n",
       " 0.15841629,\n",
       " 0.15827833,\n",
       " 0.15814073,\n",
       " 0.15800351,\n",
       " 0.15786666,\n",
       " 0.15773018,\n",
       " 0.15759407,\n",
       " 0.15745831,\n",
       " 0.15732291,\n",
       " 0.15718791,\n",
       " 0.15705328,\n",
       " 0.15691896,\n",
       " 0.15678503,\n",
       " 0.15665144,\n",
       " 0.15651819,\n",
       " 0.1563853,\n",
       " 0.15625277,\n",
       " 0.15612061,\n",
       " 0.15598875,\n",
       " 0.15585726,\n",
       " 0.15572613,\n",
       " 0.1555953,\n",
       " 0.15546486,\n",
       " 0.15533473,\n",
       " 0.15520495,\n",
       " 0.15507549,\n",
       " 0.15494636,\n",
       " 0.15481758,\n",
       " 0.15468913,\n",
       " 0.154561,\n",
       " 0.15443321,\n",
       " 0.15430573,\n",
       " 0.15417857,\n",
       " 0.15405177,\n",
       " 0.15392525,\n",
       " 0.15379907,\n",
       " 0.15367322,\n",
       " 0.15354767,\n",
       " 0.15342243,\n",
       " 0.15329753,\n",
       " 0.15317293,\n",
       " 0.1530486,\n",
       " 0.15292464,\n",
       " 0.15280099,\n",
       " 0.15267763,\n",
       " 0.15255454,\n",
       " 0.1524318,\n",
       " 0.15230937,\n",
       " 0.15218723,\n",
       " 0.15206535,\n",
       " 0.15194382,\n",
       " 0.15182257,\n",
       " 0.15170161,\n",
       " 0.15158094,\n",
       " 0.15146057,\n",
       " 0.1513405,\n",
       " 0.15122072,\n",
       " 0.15110123,\n",
       " 0.15098204,\n",
       " 0.15086313,\n",
       " 0.15074448,\n",
       " 0.15062617,\n",
       " 0.15050809,\n",
       " 0.15039033,\n",
       " 0.15027282,\n",
       " 0.15015562,\n",
       " 0.15003866,\n",
       " 0.14992201,\n",
       " 0.14980562,\n",
       " 0.1496895,\n",
       " 0.14957367,\n",
       " 0.14945811,\n",
       " 0.14934283,\n",
       " 0.14922778,\n",
       " 0.14911304,\n",
       " 0.14899857,\n",
       " 0.14888434,\n",
       " 0.14877041,\n",
       " 0.14865673,\n",
       " 0.14854331,\n",
       " 0.14843014,\n",
       " 0.14831726,\n",
       " 0.14820459,\n",
       " 0.14809223,\n",
       " 0.14798014,\n",
       " 0.14786828,\n",
       " 0.14775664,\n",
       " 0.14764529,\n",
       " 0.14753421,\n",
       " 0.14742336,\n",
       " 0.14731278,\n",
       " 0.14720243,\n",
       " 0.14709234,\n",
       " 0.14698252,\n",
       " 0.14687292,\n",
       " 0.14676356,\n",
       " 0.14665449,\n",
       " 0.1465456,\n",
       " 0.14643699,\n",
       " 0.14632864,\n",
       " 0.14622052,\n",
       " 0.14611262,\n",
       " 0.14600496,\n",
       " 0.14589758,\n",
       " 0.14579041,\n",
       " 0.14568347,\n",
       " 0.14557678,\n",
       " 0.14547029,\n",
       " 0.14536408,\n",
       " 0.14525807,\n",
       " 0.14515232,\n",
       " 0.14504677,\n",
       " 0.14494148,\n",
       " 0.14483643,\n",
       " 0.14473158,\n",
       " 0.14462696,\n",
       " 0.14452258,\n",
       " 0.1444184,\n",
       " 0.14431445,\n",
       " 0.14421074,\n",
       " 0.14410725,\n",
       " 0.144004,\n",
       " 0.14390096,\n",
       " 0.14379813,\n",
       " 0.14369552,\n",
       " 0.14359313,\n",
       " 0.14349097,\n",
       " 0.14338902,\n",
       " 0.14328729,\n",
       " 0.14318578,\n",
       " 0.14308447,\n",
       " 0.14298338,\n",
       " 0.14288251,\n",
       " 0.14278184,\n",
       " 0.1426814,\n",
       " 0.14258116,\n",
       " 0.14248113,\n",
       " 0.1423813,\n",
       " 0.1422817,\n",
       " 0.14218229,\n",
       " 0.14208308,\n",
       " 0.14198409,\n",
       " 0.14188533,\n",
       " 0.14178671,\n",
       " 0.14168835,\n",
       " 0.14159019,\n",
       " 0.1414922,\n",
       " 0.14139444,\n",
       " 0.14129685,\n",
       " 0.14119951,\n",
       " 0.14110234,\n",
       " 0.14100538,\n",
       " 0.1409086,\n",
       " 0.14081202,\n",
       " 0.14071563,\n",
       " 0.14061946,\n",
       " 0.14052346,\n",
       " 0.14042766,\n",
       " 0.14033209,\n",
       " 0.14023666,\n",
       " 0.14014144,\n",
       " 0.14004642,\n",
       " 0.13995159,\n",
       " 0.13985695,\n",
       " 0.13976251,\n",
       " 0.13966824,\n",
       " 0.13957416,\n",
       " 0.13948025,\n",
       " 0.13938656,\n",
       " 0.13929303,\n",
       " 0.13919972,\n",
       " 0.13910657,\n",
       " 0.1390136,\n",
       " 0.13892083,\n",
       " 0.13882823,\n",
       " 0.13873583,\n",
       " 0.13864359,\n",
       " 0.13855155,\n",
       " 0.13845968,\n",
       " 0.13836798,\n",
       " 0.13827647,\n",
       " 0.13818514,\n",
       " 0.13809398,\n",
       " 0.13800302,\n",
       " 0.13791221,\n",
       " 0.1378216,\n",
       " 0.13773115,\n",
       " 0.13764086,\n",
       " 0.13755074,\n",
       " 0.13746084,\n",
       " 0.13737111,\n",
       " 0.13728152,\n",
       " 0.13719212,\n",
       " 0.13710286,\n",
       " 0.13701379,\n",
       " 0.13692489,\n",
       " 0.13683619,\n",
       " 0.13674764,\n",
       " 0.13665922,\n",
       " 0.13657102,\n",
       " 0.13648295,\n",
       " 0.1363951,\n",
       " 0.13630734,\n",
       " 0.13621981,\n",
       " 0.1361324,\n",
       " 0.1360452,\n",
       " 0.13595812,\n",
       " 0.13587125,\n",
       " 0.13578451,\n",
       " 0.13569793,\n",
       " 0.13561153,\n",
       " 0.13552527,\n",
       " 0.13543919,\n",
       " 0.13535325,\n",
       " 0.13526748,\n",
       " 0.13518186,\n",
       " 0.13509642,\n",
       " 0.13501115,\n",
       " 0.13492599,\n",
       " 0.13484101,\n",
       " 0.13475621,\n",
       " 0.13467155,\n",
       " 0.13458702,\n",
       " 0.13450268,\n",
       " 0.13441847,\n",
       " 0.13433443,\n",
       " 0.13425055,\n",
       " 0.13416679,\n",
       " 0.13408324,\n",
       " 0.13399979,\n",
       " 0.13391651,\n",
       " 0.13383338,\n",
       " 0.13375041,\n",
       " 0.13366757,\n",
       " 0.1335849,\n",
       " 0.13350236,\n",
       " 0.13341998,\n",
       " 0.13333777,\n",
       " 0.13325568,\n",
       " 0.13317372,\n",
       " 0.13309194,\n",
       " 0.1330103,\n",
       " 0.13292882,\n",
       " 0.13284746,\n",
       " 0.13276625,\n",
       " 0.13268517,\n",
       " 0.13260427,\n",
       " 0.13252349,\n",
       " 0.13244288,\n",
       " 0.13236237,\n",
       " 0.13228203,\n",
       " 0.13220182,\n",
       " 0.13212176,\n",
       " 0.13204186,\n",
       " 0.13196208,\n",
       " 0.13188243,\n",
       " 0.13180295,\n",
       " 0.13172358,\n",
       " 0.13164437,\n",
       " 0.1315653,\n",
       " 0.13148636,\n",
       " 0.13140753,\n",
       " 0.13132888,\n",
       " 0.13125035,\n",
       " 0.13117196,\n",
       " 0.1310937,\n",
       " 0.13101558,\n",
       " 0.13093761,\n",
       " 0.13085975,\n",
       " 0.13078204,\n",
       " 0.13070445,\n",
       " 0.13062699,\n",
       " 0.13054968,\n",
       " 0.13047248,\n",
       " 0.13039546,\n",
       " 0.13031852,\n",
       " 0.13024175,\n",
       " 0.1301651,\n",
       " 0.13008858,\n",
       " 0.13001218,\n",
       " 0.12993594,\n",
       " 0.12985981,\n",
       " 0.12978379,\n",
       " 0.12970793,\n",
       " 0.12963217,\n",
       " 0.12955657,\n",
       " 0.12948108,\n",
       " 0.12940572,\n",
       " 0.12933049,\n",
       " 0.12925538,\n",
       " 0.1291804,\n",
       " 0.12910555,\n",
       " 0.12903082,\n",
       " 0.12895623,\n",
       " 0.12888177,\n",
       " 0.12880741,\n",
       " 0.12873319,\n",
       " 0.12865907,\n",
       " 0.12858512,\n",
       " 0.12851126,\n",
       " 0.1284375,\n",
       " 0.12836392,\n",
       " 0.12829041,\n",
       " 0.12821706,\n",
       " 0.12814382,\n",
       " 0.1280707,\n",
       " 0.1279977,\n",
       " 0.12792481,\n",
       " 0.12785205,\n",
       " 0.12777942,\n",
       " 0.1277069,\n",
       " 0.1276345,\n",
       " 0.1275622,\n",
       " 0.12749004,\n",
       " 0.127418,\n",
       " 0.12734608,\n",
       " 0.12727427,\n",
       " 0.12720257,\n",
       " 0.127131,\n",
       " 0.12705952,\n",
       " 0.12698822,\n",
       " 0.12691697,\n",
       " 0.12684587,\n",
       " 0.12677486,\n",
       " 0.12670399,\n",
       " 0.12663324,\n",
       " 0.12656258,\n",
       " 0.12649205,\n",
       " 0.12642163,\n",
       " 0.12635131,\n",
       " 0.12628111,\n",
       " 0.12621103,\n",
       " 0.12614109,\n",
       " 0.12607121,\n",
       " 0.12600146,\n",
       " 0.12593184,\n",
       " 0.12586232,\n",
       " 0.12579292,\n",
       " 0.12572362,\n",
       " 0.12565441,\n",
       " 0.12558535,\n",
       " 0.12551638,\n",
       " 0.12544753,\n",
       " 0.12537876,\n",
       " 0.12531014,\n",
       " 0.12524159,\n",
       " 0.12517317,\n",
       " 0.12510486,\n",
       " 0.12503666,\n",
       " 0.12496856,\n",
       " 0.12490055,\n",
       " 0.12483267,\n",
       " 0.1247649,\n",
       " 0.12469725,\n",
       " 0.12462967,\n",
       " 0.1245622,\n",
       " 0.12449485,\n",
       " 0.1244276,\n",
       " 0.12436044,\n",
       " 0.12429342,\n",
       " 0.1242265,\n",
       " 0.12415966,\n",
       " 0.12409294,\n",
       " 0.12402634,\n",
       " 0.12395979,\n",
       " 0.1238934,\n",
       " 0.12382709,\n",
       " 0.12376086,\n",
       " 0.12369475,\n",
       " 0.12362875,\n",
       " 0.12356284,\n",
       " 0.12349706,\n",
       " 0.12343136,\n",
       " 0.12336575,\n",
       " 0.12330028,\n",
       " 0.12323487,\n",
       " 0.12316959,\n",
       " 0.12310439,\n",
       " 0.12303928,\n",
       " 0.12297429,\n",
       " 0.1229094,\n",
       " 0.12284461,\n",
       " 0.12277992,\n",
       " 0.12271529,\n",
       " 0.12265081,\n",
       " 0.1225864,\n",
       " 0.12252209,\n",
       " 0.12245789,\n",
       " 0.12239379,\n",
       " 0.12232976,\n",
       " 0.12226585,\n",
       " 0.12220204,\n",
       " 0.12213834,\n",
       " 0.12207469,\n",
       " 0.12201114,\n",
       " 0.12194773,\n",
       " 0.12188439,\n",
       " 0.12182118,\n",
       " 0.12175801,\n",
       " 0.12169497,\n",
       " 0.121632,\n",
       " 0.12156914,\n",
       " 0.12150639,\n",
       " 0.1214437,\n",
       " 0.12138109,\n",
       " 0.12131862,\n",
       " 0.12125621,\n",
       " 0.12119393,\n",
       " 0.12113174,\n",
       " 0.1210696,\n",
       " 0.12100759,\n",
       " 0.12094563,\n",
       " 0.12088379,\n",
       " 0.12082206,\n",
       " 0.1207604,\n",
       " 0.12069882,\n",
       " 0.12063735,\n",
       " 0.12057596,\n",
       " 0.12051468,\n",
       " 0.12045345,\n",
       " 0.12039237,\n",
       " 0.12033133,\n",
       " 0.1202704,\n",
       " 0.12020953,\n",
       " 0.1201488,\n",
       " 0.12008812,\n",
       " 0.12002755,\n",
       " 0.11996706,\n",
       " 0.11990668,\n",
       " 0.11984637,\n",
       " 0.11978611,\n",
       " 0.11972599,\n",
       " 0.11966594,\n",
       " 0.11960597,\n",
       " 0.11954612,\n",
       " 0.11948633,\n",
       " 0.11942664,\n",
       " 0.11936704,\n",
       " 0.11930748,\n",
       " 0.11924806,\n",
       " 0.11918871,\n",
       " 0.11912945,\n",
       " 0.11907023,\n",
       " 0.11901115,\n",
       " 0.11895213,\n",
       " 0.11889321,\n",
       " 0.11883436,\n",
       " 0.1187756,\n",
       " 0.11871695,\n",
       " 0.11865834,\n",
       " 0.11859983,\n",
       " 0.11854141,\n",
       " 0.11848307,\n",
       " 0.11842484,\n",
       " 0.11836666,\n",
       " 0.11830854,\n",
       " 0.11825055,\n",
       " 0.11819265,\n",
       " 0.11813482,\n",
       " 0.11807704,\n",
       " ...]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on Test Set:\n",
    "We need to find A3 for all examples in the Test Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = session_1.run(A3,feed_dict={X:x_test })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = (predictions > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_test = np.sum(pred == y_test)/y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.965034965034965"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_train = session_1.run(A3,feed_dict={X:x_train })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_training = (pred_train > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_test = np.sum(pred_training == y_train)/y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99295774647887325"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Note:\n",
    "There are many in-built cost functions in Tensorflow and most of the times, we use them. But for this, we need to have our labels i.e. output variable in the form of 'one-hot vectors'. Sinse in this tutorial, we didn't have y as one-hot vectors, so I wrote the full cost function.Also, tensorflow has an in-built function to convert your labels into one-hot vectors. You can find more tutorials in this link: https://www.tensorflow.org/<br>\n",
    "\n",
    "Also, to install tensorflow:<br>\n",
    "1) Open terminal/command window.<br>\n",
    "2) Open conda terminal by using the command: conda.<br>\n",
    "3) In the conda terminal, enter the following command: conda install -c conda-forge tensorflow \n",
    "\n",
    "This will install tensorflow in your anaconda environment and finally, you can use it in the jupyter notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
